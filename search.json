[{"path":"https://garthtarr.github.io/BayesianLasso/articles/BayesianLasso.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"BayesianLasso","text":"package provides efficient sampling algorithms Bayesian Lasso regression, modified Hans PC samplers. modified Hans sampler based newly defined Lasso distribution. vignette introduces main functionality package demonstrates apply samplers real simulated data.","code":""},{"path":"https://garthtarr.github.io/BayesianLasso/articles/BayesianLasso.html","id":"simulated-example","dir":"Articles","previous_headings":"","what":"Simulated Example","title":"BayesianLasso","text":"","code":"# Simulate data set.seed(123) n <- 100 p <- 10 X <- matrix(rnorm(n * p), nrow = n) beta <- c(rep(2, 3), rep(0, p - 3)) y <- X %*% beta + rnorm(n)  # Run the modified Hans sampler output_Hans <- Modified_Hans_Gibbs(                X = X, y = y, a1=2, b1=1, u1=2, v1=1,                 nsamples=1000, beta_init= rep(1,10), lambda_init=1, sigma2_init=1, verbose=100) #> iter: 0 lambda2: 4.91771 sigma2: 0.960406 #> iter: 100 lambda2: 3.58811 sigma2: 0.997586 #> iter: 200 lambda2: 0.933625 sigma2: 0.901476 #> iter: 300 lambda2: 0.794242 sigma2: 0.991402 #> iter: 400 lambda2: 3.81367 sigma2: 1.01224 #> iter: 500 lambda2: 2.58749 sigma2: 0.906112 #> iter: 600 lambda2: 1.17227 sigma2: 1.2331 #> iter: 700 lambda2: 3.38756 sigma2: 1.27917 #> iter: 800 lambda2: 0.676862 sigma2: 0.881326 #> iter: 900 lambda2: 2.87214 sigma2: 1.21127 colMeans(output_Hans$mBeta) #>  [1]  2.05825310  1.96658242  1.85249926  0.12747215  0.06827941 -0.04384691 #>  [7]  0.07039560  0.10960198 -0.04258196  0.12511862  # Run the modified PC sampler output_PC <- Modified_PC_Gibbs(                X = X, y = y, a1=2, b1=1, u1=2, v1=1,                 nsamples=1000, lambda_init=1, sigma2_init=1, verbose=100) #> iter: 0 #> iter: 100 #> iter: 200 #> iter: 300 #> iter: 400 #> iter: 500 #> iter: 600 #> iter: 700 #> iter: 800 #> iter: 900 colMeans(output_PC$mBeta) #>  [1]  2.05197868  1.96502229  1.84986538  0.12355534  0.06721269 -0.04147922 #>  [7]  0.06621934  0.11292249 -0.03955249  0.12324313"},{"path":"https://garthtarr.github.io/BayesianLasso/articles/BayesianLasso.html","id":"data-sources","dir":"Articles","previous_headings":"","what":"Data Sources","title":"BayesianLasso","text":"vignette demonstrates usage BayesianLasso package several well-known datasets. avoid redistribution issues keep package lightweight, none datasets included directly package. Instead, provide guidance access . Diabetes: Available package loaded using data(diabetes, package = \"lars\"). Kakadu: Available package loaded using data(Kakadu, package = \"Ecdat\"). Communities Crime: dataset available UCI Machine Learning Repository. Due licensing redistribution considerations, Communities Crime dataset included package. Users can download dataset manually visiting repository following preprocessing steps outlined vignette.","code":""},{"path":"https://garthtarr.github.io/BayesianLasso/articles/BayesianLasso.html","id":"reproducing-table-1-performance-comparison","dir":"Articles","previous_headings":"","what":"Reproducing Table 1: Performance Comparison","title":"BayesianLasso","text":"section demonstrates reproduce performance comparison table manuscript, comparing mixing percentages, sampling efficiencies, runtimes Diabetes, Kakadu, Communities Crime datasets.","code":"effective_sample_size <- function(samples)  {   # Using the spectral method   N <- length(samples)      # Compute spectral density at zero using spectrum0.ar (uses AR smoothing)   spectral_density_zero <- spectrum0.ar(samples)$spec      # Compute effective sample size using variance ratio   sample_variance <- var(samples)   tau <- spectral_density_zero / sample_variance  # Integrated autocorrelation time   ESS <- N / tau   if (ESS > N) {     ESS = N   }      return(ESS) } # Load libraries library(BayesianLasso) # This code requires bayesreg  if (requireNamespace(\"monomvn\", quietly = TRUE)) {   monomvn::monomvn(...)  # use the function safely } else {   message(\"monomvn package not installed; skipping example.\") } if (requireNamespace(\"bayeslm\", quietly = TRUE)) {   bayeslm::bayeslm(...)  # use the function safely } else {   message(\"bayeslm package not installed; skipping example.\") } if (requireNamespace(\"rstan\", quietly = TRUE)) {   rstan::rstan(...)  # use the function safely } else {   message(\"rstan package not installed; skipping example.\") } if (requireNamespace(\"bayesreg\", quietly = TRUE)) {   bayesreg::bayesreg(...)  # use the function safely } else {   message(\"bayesreg package not installed; skipping example.\") }   # Example: dataset_name <- \"diabetes2\"  if (dataset_name==\"diabetes2\")  {   if (!requireNamespace(\"lars\", quietly = TRUE)) {     install.packages(\"lars\")   }   data(diabetes, package = \"lars\")    y = diabetes$y   x = diabetes$x   inds = 1:ncol(x)      # Normalizing and scaling the dataset by function normalize()   norm = normalize(y,x, scale = TRUE)   x = norm$mX   x <- model.matrix(~.^2, data=data.frame(x=x))[,-1]   y <- norm$vy    }  if (dataset_name==\"Kakadu2\")  {   if (!requireNamespace(\"Ecdat\", quietly = TRUE)) {     install.packages(\"Ecdat\")   }   dat <- data(\"Kakadu\", package = \"Ecdat\")       # Get y vector and X matrix   y <- as.vector(dat$income)   x <- dat[,c(2:21,23)]        x <- model.matrix(~.^2,data=x)[,-1] }  # Make sure Crime.csv (or comData.Rdata) is downloaded manually before running this if (dataset_name==\"Crime\")  {   rdata_path <- system.file(\"extdata\", \"comData.Rdata\", package = \"BayesianLasso\")   load(rdata_path)   # Show only if file exists   crime_path <- \"Crime.csv\"   if (file.exists(crime_path)) {     crime_data <- read.csv(crime_path)   } else {     message(\"Please download 'communities.data' from the UCI Machine Learning Repository and convert to Crime.csv.\")   }       mX <- t(t(X) %>% na.omit())     mX <- mX[,-which(colnames(X)%in%c(\"ownHousQrange\",\"rentUpperQ\"))]   varnames <- colnames(mX)   vy <- Y[,\"murders\"]   x <- mX   y <- vy   inds = 1:ncol(x) }   # Set prior hyperparameter constants a1 = 1.0E-2  # Prior shape for sigma2 b1 = 1.0E-2  # Prior scale for sigma2 u1 = 1.0E-2  # Prior shape for lambda2 v1 = 1.0E-2  # Prior scale for lambda2  # Initial values for lambda2 and sigma2 lambda2_init  = 10 lambda_init = sqrt(lambda2_init) sigma2_init = 1  # Number of samples to run the MCMC nburn = 1000 nsamples = 5000 inds_use = (nburn + 1):nsamples N = length(inds_use)  # To store elapsed time and results of the PC sampler vtime_val_PC = c() results_PC <- NULL  # to be initialized after the first run  # Running the modified PC sampler 5 times and taking the average of the results across runs. for (i in 1:5) {   time_val <- system.time({     res_PC <- Modified_PC_Gibbs(       X = x, y = y, a1, b1, u1, v1,       nsamples,       lambda_init = lambda_init,       sigma2_init = sigma2_init,       verbose = 1000     )   })[3]    vtime_val_PC[i] <- time_val    # Initialize accumulators after first run   if (is.null(results_PC)) {     results_PC <- list(       mBeta = res_PC$mBeta,       vsigma2 = res_PC$vsigma2,       vlambda2 = res_PC$vlambda2     )   } else {     results_PC$mBeta    <- results_PC$mBeta + res_PC$mBeta     results_PC$vsigma2  <- results_PC$vsigma2 + res_PC$vsigma2     results_PC$vlambda2 <- results_PC$vlambda2 + res_PC$vlambda2   } }  # Take averages mBeta = (res_PC$mBeta)/5 vsigma2 = (res_PC$vsigma2)/5 vlambda2 = (res_PC$vlambda2)/5 time_val_PC = mean(vtime_val_PC)  # Compute effective sample sizes     vESS <- c()   for(j in 1:p) {     vESS[j] <- effective_sample_size(mBeta[inds_use,j])   }   Ef_PC = median(vESS)/time_val_PC      ESS_sigma2_PC  = effective_sample_size(as.vector(vsigma2[inds_use]))   Ef_sigma2_PC = ESS_sigma2_PC/time_val_PC      ESS_lambda2_PC  = effective_sample_size(as.vector(vlambda2[inds_use]))   Ef_lambda2_PC = ESS_lambda2_PC/time_val_PC      stat_vec_PC = c(     100*median(vESS)/N,     Ef_PC,     100*ESS_sigma2_PC/N,     Ef_sigma2_PC,     100*ESS_lambda2_PC/N,     Ef_lambda2_PC,     time_val_PC)    name_vals = c(\"mix_beta\", \"eff_beta\", \"mix_sigma2\", \"eff_sigma2\", \"mix_lambda2\", \"eff_lambda2\", \"time\") names(stat_vec_PC) = name_vals    # To store elapsed time and results of the Hans sampler vtime_val_Hans = c() results_Hans <- NULL  # to be initialized after the first run  # Running the modified Hans sampler 5 times and taking the average of the results across runs. for (i in 1:5) {   time_val <- system.time({     res_Hans <- Modified_Hans_Gibbs(       X = x, y = y, a1, b1, u1, v1,       nsamples,       beta_init = as.vector(colMeans(mBeta)),       lambda_init = lambda_init,       sigma2_init = sigma2_init,       verbose = 1000     )   })[3]    vtime_val_Hans[i] <- time_val    # Initialize accumulators after first run   if (is.null(results_Hans)) {     results_Hans <- list(       mBeta = res_Hans$mBeta,       vsigma2 = res_Hans$vsigma2,       vlambda2 = res_Hans$vlambda2     )   } else {     results_Hans$mBeta    <- results_Hans$mBeta + res_Hans$mBeta     results_Hans$vsigma2  <- results_Hans$vsigma2 + res_Hans$vsigma2     results_Hans$vlambda2 <- results_Hans$vlambda2 + res_Hans$vlambda2   } }  # Take averages mBeta = (results_Hans$mBeta)/5 vsigma2 = (results_Hans$vsigma2)/5 vlambda2 = (results_Hans$vlambda2)/5 time_val_Hans = mean(vtime_val_Hans)  # Compute effective sample sizes   vESS <- c()   for(j in 1:p) {     vESS[j] <- effective_sample_size(mBeta[inds_use,j])   }   Ef_Hans = median(vESS)/time_val_Hans      ESS_sigma2_Hans  = effective_sample_size(as.vector(vsigma2[inds_use]))   Ef_sigma2_Hans = ESS_sigma2_Hans/time_val_Hans      ESS_lambda2_Hans  = effective_sample_size(as.vector(vlambda2[inds_use]))   Ef_lambda2_Hans = ESS_lambda2_Hans/time_val_Hans      stat_vec_Hans = c(     100*median(vESS)/N,     Ef_Hans,     100*ESS_sigma2_Hans/N,     Ef_sigma2_Hans,     100*ESS_lambda2_Hans/N,     Ef_lambda2_Hans,     time_val_Hans)    name_vals = c(\"mix_beta\", \"eff_beta\", \"mix_sigma2\", \"eff_sigma2\", \"mix_lambda2\", \"eff_lambda2\", \"time\") names(stat_vec_Hans) = name_vals"},{"path":"https://garthtarr.github.io/BayesianLasso/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"John Ormerod. Author, copyright holder. Mohammad Javad Davoudabadi. Author, maintainer, copyright holder. Garth Tarr. Author, copyright holder. Samuel Mueller. Author, copyright holder. Jonathon Tidswell. Author, copyright holder.","code":""},{"path":"https://garthtarr.github.io/BayesianLasso/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Ormerod J, Davoudabadi M, Tarr G, Mueller S, Tidswell J (2025). Bayesian Lasso Regression Tools Lasso Distribution. R package version 0.3.0, https://garthtarr.github.io/BayesianLasso/.","code":"@Manual{,   title = {Bayesian Lasso Regression and Tools for the Lasso Distribution},   author = {John Ormerod and Mohammad Javad Davoudabadi and Garth Tarr and Samuel Mueller and Jonathon Tidswell},   year = {2025},   note = {R package version 0.3.0},   url = {https://garthtarr.github.io/BayesianLasso/}, }"},{"path":"https://garthtarr.github.io/BayesianLasso/index.html","id":"bayesianlasso","dir":"","previous_headings":"","what":"Bayesian Lasso Regression and Tools for the Lasso Distribution","title":"Bayesian Lasso Regression and Tools for the Lasso Distribution","text":"BayesianLasso R package efficient Bayesian inference sparse linear regression models using Bayesian Lasso. includes optimized Gibbs sampling algorithms utilities working Lasso distribution.","code":""},{"path":"https://garthtarr.github.io/BayesianLasso/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Bayesian Lasso Regression and Tools for the Lasso Distribution","text":"can install development version BayesianLasso GitHub :","code":"# install.packages(\"pak\") pak::pak(\"garthtarr/BayesianLasso\")"},{"path":"https://garthtarr.github.io/BayesianLasso/index.html","id":"features","dir":"","previous_headings":"","what":"Features","title":"Bayesian Lasso Regression and Tools for the Lasso Distribution","text":"Efficient Gibbs samplers Bayesian Lasso (e.g., Modified_Hans_Gibbs, Modified_PC_Gibbs) Support drawing Lasso distribution Utilities computing moments densities","code":""},{"path":"https://garthtarr.github.io/BayesianLasso/index.html","id":"example-usage","dir":"","previous_headings":"","what":"Example Usage","title":"Bayesian Lasso Regression and Tools for the Lasso Distribution","text":"basic examples show solve common problem: Modified_Hans_Gibbs() function returns list following components: mBeta: MCMC samples regression coefficients ð›ƒ\\boldsymbol{\\beta}, stored matrix nsamples rows p columns. vsigma2: MCMC samples error variance Ïƒ2\\sigma^2. vlambda2: MCMC samples global shrinkage parameter Î»2\\lambda^2. mA, mB, mC: Matrices containing MCMC samples Lasso distribution parameters AjA_j, BjB_j, CjC_j coefficient Î²j\\beta_j, row corresponds one MCMC iteration column regression coefficient.","code":"library(BayesianLasso) ## basic example code  # Simulated data set.seed(123) X <- matrix(rnorm(100), 20, 5) y <- rnorm(20) beta_init <- rep(1, 5)  # Run modified Hans Gibbs sampler result <- Modified_Hans_Gibbs(   X = X,   y = y,   a1 = 0.01,   b1 = 0.01,   u1 = 0.01,   v1 = 0.01,   nsamples = 100,   beta_init = beta_init,   lambda_init = 0.1,   sigma2_init = 1,   verbose = 0 )  str(result) #> List of 6 #>  $ mBeta   : num [1:100, 1:5] 0.2441 0.2277 0.2478 -0.1356 -0.0692 ... #>  $ vsigma2 : num [1:100, 1] 0.913 0.767 0.704 0.747 0.623 ... #>  $ vlambda2: num [1:100, 1] 34.96 87.38 9.41 53.49 68.44 ... #>  $ mA      : num [1:100, 1:5] 18.4 20.1 24 26.1 24.6 ... #>  $ mB      : num [1:100, 1:5] 5.67 3.15 3 2.11 2.49 ... #>  $ mC      : num [1:100, 1:5] 0.1 6.19 10.68 3.66 8.46 ..."},{"path":"https://garthtarr.github.io/BayesianLasso/index.html","id":"lasso-distribution-functions","dir":"","previous_headings":"","what":"Lasso Distribution Functions","title":"Bayesian Lasso Regression and Tools for the Lasso Distribution","text":"package provides functions working Lasso distribution: zlasso(): Normalizing constant dlasso(): Density function plasso(): CDF qlasso(): Quantile function rlasso(): Random generation elasso(): Expected value vlasso(): Variance mlasso(): Mode MillsRatio(): Mills ratio","code":""},{"path":"https://garthtarr.github.io/BayesianLasso/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Bayesian Lasso Regression and Tools for the Lasso Distribution","text":"use package work, please cite appropriately. Citation information can found using:","code":"citation(\"BayesianLasso\") #> To cite package 'BayesianLasso' in publications use: #>  #>   Ormerod J, Davoudabadi M, Tarr G, Mueller S, Tidswell J (2025). #>   _Bayesian Lasso Regression and Tools for the Lasso Distribution_. R #>   package version 0.3.0, <https://garthtarr.github.io/BayesianLasso/>. #>  #> A BibTeX entry for LaTeX users is #>  #>   @Manual{, #>     title = {Bayesian Lasso Regression and Tools for the Lasso Distribution}, #>     author = {John Ormerod and Mohammad Javad Davoudabadi and Garth Tarr and Samuel Mueller and Jonathon Tidswell}, #>     year = {2025}, #>     note = {R package version 0.3.0}, #>     url = {https://garthtarr.github.io/BayesianLasso/}, #>   }"},{"path":"https://garthtarr.github.io/BayesianLasso/reference/LassoDistribution.html","id":null,"dir":"Reference","previous_headings":"","what":"The Lasso Distribution â€” LassoDistribution","title":"The Lasso Distribution â€” LassoDistribution","text":"Provides functions related Lasso distribution, including normalizing constant, probability density function, cumulative distribution function, quantile function, random number generation given parameters , b, c. Additional utilities include Mills ratio, expected value, variance distribution. package also implements modified versions Hans Parkâ€“Casella Gibbs sampling algorithms Bayesian Lasso regression.","code":""},{"path":"https://garthtarr.github.io/BayesianLasso/reference/LassoDistribution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The Lasso Distribution â€” LassoDistribution","text":"","code":"zlasso(a, b, c, logarithm) dlasso(x, a, b, c, logarithm) plasso(q, a, b, c) qlasso(p, a, b, c) rlasso(n, a, b, c) elasso(a, b, c) vlasso(a, b, c) mlasso(a, b, c) MillsRatio(d) Modified_Hans_Gibbs(X, y, a1, b1, u1, v1,               nsamples, beta_init, lambda_init, sigma2_init, verbose) Modified_PC_Gibbs(X, y, a1, b1, u1, v1,                nsamples, lambda_init, sigma2_init, verbose)"},{"path":"https://garthtarr.github.io/BayesianLasso/reference/LassoDistribution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Lasso Distribution â€” LassoDistribution","text":"x, q Vector quantiles (vectorized). p Vector probabilities. Vector precision parameter must non-negative. b Vector set parameter. c Vector tuning parameter must non-negative values. n Number observations. logarithm Logical. TRUE, probabilities returned log scale. d scalar numeric value. Represents point Mills ratio evaluated. X Design matrix (numeric matrix). y Response vector (numeric vector). a1 Shape parameter prior \\(\\lambda^2\\). b1 Rate parameter prior \\(\\lambda^2\\). u1 Shape parameter prior \\(\\sigma^2\\). v1 Rate parameter prior \\(\\sigma^2\\). nsamples Number Gibbs samples draw. beta_init Initial value model parameter \\(\\beta\\). lambda_init Initial value shrinkage parameter \\(\\lambda^2\\). sigma2_init Initial value error variance \\(\\sigma^2\\). verbose Integer. greater 0, progress printed every verbose iterations sampling. Set 0 suppress output.","code":""},{"path":"https://garthtarr.github.io/BayesianLasso/reference/LassoDistribution.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"The Lasso Distribution â€” LassoDistribution","text":"zlasso, dlasso, plasso, qlasso, rlasso, elasso, vlasso, mlasso, MillsRatio:   return corresponding scalar vector values related Lasso distribution numeric value representing Mills ratio. Modified_Hans_Gibbs: returns list containing: mBeta Matrix MCMC samples regression coefficients \\(\\beta\\), nsamples rows p columns. vsigma2 Vector MCMC samples error variance \\(\\sigma^2\\). vlambda2 Vector MCMC samples shrinkage parameter \\(\\lambda^2\\). mA Matrix sampled values parameter \\(a_j\\) Lasso distribution \\(\\beta_j\\). mB Matrix sampled values parameter \\(b_j\\) Lasso distribution \\(\\beta_j\\). mC Matrix sampled values parameter \\(c_j\\) Lasso distribution \\(\\beta_j\\). Modified_PC_Gibbs: returns list containing: mBeta Matrix MCMC samples regression coefficients \\(\\beta\\). vsigma2 Vector MCMC samples error variance \\(\\sigma^2\\). vlambda2 Vector MCMC samples shrinkage parameter \\(\\lambda^2\\). mM Matrix estimated means full conditional distributions \\(\\beta_j\\). mV Matrix estimated variances full conditional distributions \\(\\beta_j\\). va_til Vector estimated shape parameters full conditional inverse-gamma distribution \\(\\sigma^2\\). vb_til Vector estimated rate parameters full conditional inverse-gamma distribution \\(\\sigma^2\\). vu_til Vector estimated shape parameters full conditional inverse-gamma distribution \\(\\lambda^2\\). vv_til Vector estimated rate parameters full conditional inverse-gamma distribution \\(\\lambda^2\\).","code":""},{"path":"https://garthtarr.github.io/BayesianLasso/reference/LassoDistribution.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"The Lasso Distribution â€” LassoDistribution","text":"\\(X \\sim \\text{Lasso}(, b, c)\\) density function : $$ p(x;,b,c) = Z^{-1} \\exp\\left(-\\frac{1}{2} x^2 + bx - c|x| \\right) $$ \\(x \\\\mathbb{R}\\), \\(> 0\\), \\(b \\\\mathbb{R}\\), \\(c > 0\\), \\(Z\\) normalizing constant. details included CDF, quantile function, normalizing constant original documentation.","code":""},{"path":[]},{"path":"https://garthtarr.github.io/BayesianLasso/reference/LassoDistribution.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"The Lasso Distribution â€” LassoDistribution","text":"","code":"a <- 2; b <- 1; c <- 3 x <- seq(-3, 3, length.out = 1000) plot(x, dlasso(x, a, b, c, logarithm = FALSE), type = 'l')   r <- rlasso(1000, a, b, c) hist(r, breaks = 50, probability = TRUE, col = \"grey\", border = \"white\") lines(x, dlasso(x, a, b, c, logarithm = FALSE), col = \"blue\")   plasso(0, a, b, c) #> [1] 0.3739435 qlasso(0.25, a, b, c) #> [1] -0.08945799 elasso(a, b, c) #> [1] 0.1218306 vlasso(a, b, c) #> [1] 0.1287739 mlasso(a, b, c) #> [1] 0 MillsRatio(2) #> [1] 0.4213692     # The Modified_Hans_Gibbs() function uses the Lasso distribution to draw  # samples from the full conditional distribution of the regression coefficients.  y <- 1:20 X <- matrix(c(1:20,12:31,7:26),20,3,byrow = TRUE)  a1 <- b1 <- u1 <- v1 <- 0.01 sigma2_init <- 1 lambda_init <- 0.1 beta_init <- rep(1, ncol(X)) nsamples <- 1000 verbose <- 100  Output_Hans <- Modified_Hans_Gibbs(                 X, y, a1, b1, u1, v1,                 nsamples, beta_init, lambda_init, sigma2_init, verbose ) #> iter: 0 lambda2: 111.787 sigma2: 135.243 #> iter: 100 lambda2: 10.8956 sigma2: 25.884 #> iter: 200 lambda2: 61.9234 sigma2: 20.4167 #> iter: 300 lambda2: 94.7265 sigma2: 39.2217 #> iter: 400 lambda2: 72.3648 sigma2: 15.074 #> iter: 500 lambda2: 50.5227 sigma2: 15.9175 #> iter: 600 lambda2: 135.66 sigma2: 20.0574 #> iter: 700 lambda2: 38.0999 sigma2: 30.6545 #> iter: 800 lambda2: 2.58036 sigma2: 22.0707 #> iter: 900 lambda2: 25.2982 sigma2: 45.5914  colMeans(Output_Hans$mBeta) #> [1]  0.3472431 -0.2546945  0.5260358 mean(Output_Hans$vlambda2) #> [1] 70.99966   Output_PC <- Modified_PC_Gibbs(                X, y, a1, b1, u1, v1,                 nsamples, lambda_init, sigma2_init, verbose) #> iter: 0 #> iter: 100 #> iter: 200 #> iter: 300 #> iter: 400 #> iter: 500 #> iter: 600 #> iter: 700 #> iter: 800 #> iter: 900  colMeans(Output_PC$mBeta) #> [1]  0.32005246 -0.09033226  0.38869904 mean(Output_PC$vlambda2) #> [1] 69.80043"},{"path":"https://garthtarr.github.io/BayesianLasso/reference/normalize.html","id":null,"dir":"Reference","previous_headings":"","what":"Normalize Response and Covariates â€” normalize","title":"Normalize Response and Covariates â€” normalize","text":"function centers (optionally) scales response vector column design matrix using population variance. used prepare data Bayesian Lasso regression.","code":""},{"path":"https://garthtarr.github.io/BayesianLasso/reference/normalize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Normalize Response and Covariates â€” normalize","text":"","code":"normalize(y, X, scale = TRUE)"},{"path":"https://garthtarr.github.io/BayesianLasso/reference/normalize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Normalize Response and Covariates â€” normalize","text":"y numeric response vector. X numeric matrix data frame covariates (design matrix). scale Logical; TRUE, variables scaled unit population variance (default TRUE).","code":""},{"path":"https://garthtarr.github.io/BayesianLasso/reference/normalize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Normalize Response and Covariates â€” normalize","text":"list following elements: vy: Normalized response vector. mX: Normalized design matrix. mu.y: Mean response vector. sigma2.y: Population variance response vector. mu.x: Vector column means X. sigma2.x: Vector population variances columns X.","code":""},{"path":"https://garthtarr.github.io/BayesianLasso/reference/normalize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Normalize Response and Covariates â€” normalize","text":"","code":"set.seed(1) X <- matrix(rnorm(100 * 10), 100, 10) beta <- c(2, -3, rep(0, 8)) y <- as.vector(X %*% beta + rnorm(100)) norm_result <- normalize(y, X)"},{"path":[]},{"path":[]},{"path":[]},{"path":"https://garthtarr.github.io/BayesianLasso/news/index.html","id":"bayesianlasso-010","dir":"Changelog","previous_headings":"","what":"BayesianLasso 0.1.0","title":"BayesianLasso 0.1.0","text":"Initial CRAN submission.","code":""}]
